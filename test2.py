#!/usr/bin/env python3

import requests
import hashlib
import json
import csv
from datetime import datetime
from bs4 import BeautifulSoup
from html2text import html2text
import os
from collections import Counter

class FlomoAnalyzer:
    def __init__(self, token):
        self.token = token
        self.salt = "dbbc3dd73364b4084c3a69346e0ce2b2"
        self.base_url = "https://flomoapp.com/api/v1/memo/updated/"
        
    def get_memos_page(self, latest_slug=None, latest_updated_at=None, limit=200):
        """è·å–ä¸€é¡µå¤‡å¿˜å½•æ•°æ®"""
        current_timestamp = str(int(datetime.now().timestamp()))
        
        params = {
            "limit": str(limit),
            "tz": "8:0", 
            "timestamp": current_timestamp,
            "api_key": "flomo_web",
            "app_version": "4.0",
            "platform": "web",
            "webp": "1"
        }
        
        if latest_slug and latest_updated_at:
            params.update({
                "latest_slug": latest_slug,
                "latest_updated_at": str(latest_updated_at)
            })
        
        # ç”Ÿæˆç­¾å
        param_str = "&".join([f"{k}={v}" for k, v in sorted(params.items())])
        sign = hashlib.md5((param_str + self.salt).encode("utf-8")).hexdigest()
        params["sign"] = sign
        
        headers = {"Authorization": self.token}
        
        try:
            response = requests.get(self.base_url, params=params, headers=headers)
            if response.status_code == 200:
                data = response.json()
                if data.get("code") == 0:
                    return data.get("data", [])
            return None
        except Exception as e:
            print(f"è¯·æ±‚å¤±è´¥: {e}")
            return None
    
    def get_all_memos(self):
        """è·å–æ‰€æœ‰å¤‡å¿˜å½•"""
        all_memos = []
        latest_slug = None
        latest_updated_at = None
        page = 1
        
        while True:
            print(f"æ­£åœ¨è·å–ç¬¬ {page} é¡µæ•°æ®...")
            memos = self.get_memos_page(latest_slug, latest_updated_at)
            
            if not memos:
                break
                
            all_memos.extend(memos)
            print(f"ç¬¬ {page} é¡µè·å–åˆ° {len(memos)} æ¡å¤‡å¿˜å½•")
            
            if len(memos) < 200:  # æœ€åä¸€é¡µ
                break
                
            # è®¾ç½®ä¸‹ä¸€é¡µå‚æ•°
            last_memo = memos[-1]
            latest_slug = last_memo["slug"]
            latest_updated_at = int(datetime.fromisoformat(last_memo["updated_at"]).timestamp())
            page += 1
        
        print(f"âœ… æ€»å…±è·å–åˆ° {len(all_memos)} æ¡å¤‡å¿˜å½•")
        return all_memos
    
    def parse_memo_content(self, memo):
        """è§£æå¤‡å¿˜å½•å†…å®¹"""
        content = memo.get('content', '')
        
        # è½¬æ¢ä¸ºçº¯æ–‡æœ¬
        soup = BeautifulSoup(content, 'html.parser')
        text = soup.get_text(separator='\n', strip=True)
        
        # è½¬æ¢ä¸º Markdown
        markdown = html2text(content).strip()
        
        return {
            'slug': memo.get('slug'),
            'created_at': memo.get('created_at'),
            'updated_at': memo.get('updated_at'),
            'original_html': content,
            'plain_text': text,
            'markdown': markdown,
            'word_count': len(text),
            'tags': self.extract_tags(content)
        }
    
    def extract_tags(self, content):
        """æå–æ ‡ç­¾"""
        import re
        # æå– #æ ‡ç­¾
        tags = re.findall(r'#([^\s#]+)', content)
        return tags
    
    def analyze_memos(self, memos):
        """åˆ†æå¤‡å¿˜å½•æ•°æ®"""
        parsed_memos = [self.parse_memo_content(memo) for memo in memos]
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_count = len(parsed_memos)
        total_words = sum(memo['word_count'] for memo in parsed_memos)
        
        # æŒ‰å¹´æœˆç»Ÿè®¡
        date_counter = Counter()
        tag_counter = Counter()
        
        for memo in parsed_memos:
            if memo['created_at']:
                date = memo['created_at'][:7]  # YYYY-MM
                date_counter[date] += 1
            
            for tag in memo['tags']:
                tag_counter[tag] += 1
        
        # æ—¶é—´èŒƒå›´
        dates = [memo['created_at'] for memo in parsed_memos if memo['created_at']]
        earliest = min(dates) if dates else "N/A"
        latest = max(dates) if dates else "N/A"
        
        analysis = {
            'total_memos': total_count,
            'total_words': total_words,
            'avg_words_per_memo': round(total_words / total_count, 2) if total_count > 0 else 0,
            'date_range': f"{earliest} åˆ° {latest}",
            'monthly_distribution': dict(date_counter.most_common()),
            'top_tags': dict(tag_counter.most_common(20)),
            'parsed_memos': parsed_memos
        }
        
        return analysis
    
    def export_to_csv(self, analysis, filename="flomo_export.csv"):
        """å¯¼å‡ºåˆ°CSV"""
        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
            fieldnames = ['slug', 'created_at', 'updated_at', 'plain_text', 'markdown', 'word_count', 'tags']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            
            writer.writeheader()
            for memo in analysis['parsed_memos']:
                writer.writerow({
                    'slug': memo['slug'],
                    'created_at': memo['created_at'],
                    'updated_at': memo['updated_at'],
                    'plain_text': memo['plain_text'],
                    'markdown': memo['markdown'],
                    'word_count': memo['word_count'],
                    'tags': ','.join(memo['tags'])
                })
        
        print(f"âœ… æ•°æ®å·²å¯¼å‡ºåˆ° {filename}")
    
    def export_to_json(self, analysis, filename="flomo_export.json"):
        """å¯¼å‡ºåˆ°JSON"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… æ•°æ®å·²å¯¼å‡ºåˆ° {filename}")
    
    def print_analysis(self, analysis):
        """æ‰“å°åˆ†æç»“æœ"""
        print(f"\n{'='*60}")
        print(f"ğŸ“Š Flomo æ•°æ®åˆ†ææŠ¥å‘Š")
        print(f"{'='*60}")
        print(f"ğŸ“ æ€»å¤‡å¿˜å½•æ•°é‡: {analysis['total_memos']:,}")
        print(f"ğŸ“– æ€»å­—æ•°: {analysis['total_words']:,}")
        print(f"ğŸ“„ å¹³å‡æ¯æ¡å­—æ•°: {analysis['avg_words_per_memo']}")
        print(f"ğŸ“… æ—¶é—´èŒƒå›´: {analysis['date_range']}")
        
        print(f"\nğŸ·ï¸  çƒ­é—¨æ ‡ç­¾ (Top 10):")
        for tag, count in list(analysis['top_tags'].items())[:10]:
            print(f"   #{tag}: {count} æ¬¡")
        
        print(f"\nğŸ“ˆ æœˆåº¦åˆ†å¸ƒ (Top 10):")
        for month, count in list(analysis['monthly_distribution'].items())[:10]:
            print(f"   {month}: {count} æ¡")
        
        print(f"\nğŸ’¡ æœ€æ–°çš„ 5 æ¡å¤‡å¿˜å½•:")
        latest_memos = sorted(analysis['parsed_memos'], 
                            key=lambda x: x['created_at'] or '', reverse=True)[:5]
        for i, memo in enumerate(latest_memos, 1):
            preview = memo['plain_text'][:100] + "..." if len(memo['plain_text']) > 100 else memo['plain_text']
            print(f"   {i}. [{memo['created_at']}] {preview}")

def main():
    # é…ç½®ä½ çš„token
    TOKEN = "Bearer 6782846|pguJkOHgJ21KYW4oHfrEF0syJvHRygKI5a53Mitf"
    
    analyzer = FlomoAnalyzer(TOKEN)
    
    print("ğŸš€ å¼€å§‹è·å–å’Œåˆ†æ Flomo æ•°æ®...")
    
    # è·å–æ‰€æœ‰å¤‡å¿˜å½•
    memos = analyzer.get_all_memos()
    
    if not memos:
        print("âŒ æœªèƒ½è·å–åˆ°æ•°æ®")
        return
    
    # åˆ†ææ•°æ®
    print("\nğŸ“Š æ­£åœ¨åˆ†ææ•°æ®...")
    analysis = analyzer.analyze_memos(memos)
    
    # æ˜¾ç¤ºåˆ†æç»“æœ
    analyzer.print_analysis(analysis)
    
    # å¯¼å‡ºæ•°æ®
    print(f"\nğŸ’¾ æ­£åœ¨å¯¼å‡ºæ•°æ®...")
    analyzer.export_to_csv(analysis)
    analyzer.export_to_json(analysis)
    
    print(f"\nğŸ‰ å®Œæˆï¼ä½ çš„ Flomo æ•°æ®å·²æˆåŠŸåˆ†æå¹¶å¯¼å‡ºã€‚")

if __name__ == "__main__":
    main()