#!/usr/bin/env python3

import requests
import hashlib
import json
from datetime import datetime
from bs4 import BeautifulSoup
from html2text import html2text
import time

class FlomoCompleteAPI:
    def __init__(self, token):
        self.token = token
        self.salt = "dbbc3dd73364b4084c3a69346e0ce2b2"
        self.base_url = "https://flomoapp.com/api/v1"
        
    def _generate_params(self, extra_params=None):
        """ç”ŸæˆAPIå‚æ•°å’Œç­¾å"""
        params = {
            "timestamp": str(int(datetime.now().timestamp())),
            "api_key": "flomo_web",
            "app_version": "4.0",
            "platform": "web",
            "webp": "1"
        }
        
        if extra_params:
            params.update(extra_params)
        
        # ç”Ÿæˆç­¾å
        param_str = "&".join([f"{k}={v}" for k, v in sorted(params.items())])
        sign = hashlib.md5((param_str + self.salt).encode("utf-8")).hexdigest()
        params["sign"] = sign
        
        return params
    
    def get_all_memos(self, limit_per_page=200):
        """è·å–æ‰€æœ‰å¤‡å¿˜å½•"""
        all_memos = []
        latest_slug = None
        latest_updated_at = None
        page = 1
        
        while True:
            print(f"æ­£åœ¨è·å–ç¬¬ {page} é¡µæ•°æ®...")
            
            search_params = {"limit": str(limit_per_page), "tz": "8:0"}
            
            # æ·»åŠ åˆ†é¡µå‚æ•°
            if latest_slug and latest_updated_at:
                search_params.update({
                    "latest_slug": latest_slug,
                    "latest_updated_at": str(latest_updated_at)
                })
            
            params = self._generate_params(search_params)
            headers = {"Authorization": self.token}
            
            try:
                response = requests.get(f"{self.base_url}/memo/updated/", 
                                     params=params, headers=headers)
                
                if response.status_code == 200:
                    data = response.json()
                    if data.get("code") == 0:
                        memos = data.get("data", [])
                        
                        if not memos:
                            break
                        
                        all_memos.extend(memos)
                        print(f"ç¬¬ {page} é¡µè·å– {len(memos)} æ¡å¤‡å¿˜å½•")
                        
                        # å¦‚æœè¿™é¡µç»“æœå°‘äºé™åˆ¶æ•°é‡ï¼Œè¯´æ˜æ²¡æœ‰æ›´å¤šæ•°æ®äº†
                        if len(memos) < limit_per_page:
                            break
                        
                        # è®¾ç½®ä¸‹ä¸€é¡µå‚æ•°
                        last_memo = memos[-1]
                        latest_slug = last_memo["slug"]
                        latest_updated_at = int(datetime.fromisoformat(last_memo["updated_at"]).timestamp())
                        
                        page += 1
                        # time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«
                    else:
                        print(f"APIé”™è¯¯: {data.get('message')}")
                        break
                else:
                    print(f"HTTPé”™è¯¯: {response.status_code}")
                    break
                    
            except Exception as e:
                print(f"è¯·æ±‚å¼‚å¸¸: {e}")
                break
        
        print(f"âœ… æ€»å…±è·å–åˆ° {len(all_memos)} æ¡å¤‡å¿˜å½•")
        return all_memos
    
    def get_memo_recommendations(self, memo_slug, rec_type=1, no_same_tag=0):
        """
        è·å–å¤‡å¿˜å½•çš„ç›¸å…³æ¨è
        
        Args:
            memo_slug: å¤‡å¿˜å½•çš„slugæ ‡è¯†ç¬¦
            rec_type: æ¨èç±»å‹ (é»˜è®¤ä¸º1)
            no_same_tag: æ˜¯å¦æ’é™¤ç›¸åŒæ ‡ç­¾ (0=ä¸æ’é™¤, 1=æ’é™¤)
        """
        try:
            params = self._generate_params({
                "type": str(rec_type),
                "no_same_tag": str(no_same_tag)
            })
            
            headers = {"Authorization": self.token}
            url = f"{self.base_url}/memo/{memo_slug}/recommended"
            
            print(f"ğŸ”— è·å–å¤‡å¿˜å½• {memo_slug} çš„ç›¸å…³æ¨è...")
            
            response = requests.get(url, params=params, headers=headers)
            
            if response.status_code == 200:
                data = response.json()
                if data.get("code") == 0:
                    recommendations = data.get("data", [])
                    print(f"âœ… æ‰¾åˆ° {len(recommendations)} æ¡ç›¸å…³å¤‡å¿˜å½•")
                    return recommendations
                else:
                    print(f"âŒ APIé”™è¯¯: {data.get('message')}")
                    return []
            else:
                print(f"âŒ HTTPé”™è¯¯: {response.status_code}")
                return []
                
        except Exception as e:
            print(f"ğŸ’¥ è·å–æ¨èå¤±è´¥: {e}")
            return []
    
    def analyze_memo_relationships(self, memo_slug):
        """åˆ†æå¤‡å¿˜å½•çš„å…³è”å…³ç³»"""
        recommendations = self.get_memo_recommendations(memo_slug)
        
        if not recommendations:
            return None
        
        # åˆ†æç›¸ä¼¼åº¦åˆ†å¸ƒ
        similarities = [float(rec["similarity"]) for rec in recommendations]
        
        # åˆ†ææ ‡ç­¾åˆ†å¸ƒ
        all_tags = []
        for rec in recommendations:
            tags = rec["memo"].get("tags", [])
            all_tags.extend(tags)
        
        from collections import Counter
        tag_counter = Counter(all_tags)
        
        # åˆ†ææ—¶é—´åˆ†å¸ƒ
        dates = []
        for rec in recommendations:
            created_at = rec["memo"].get("created_at")
            if created_at:
                dates.append(created_at[:7])  # YYYY-MM
        
        date_counter = Counter(dates)
        
        analysis = {
            "total_recommendations": len(recommendations),
            "similarity_stats": {
                "max": max(similarities),
                "min": min(similarities),
                "avg": sum(similarities) / len(similarities)
            },
            "top_tags": dict(tag_counter.most_common(10)),
            "date_distribution": dict(date_counter.most_common(12)),
            "high_similarity_count": len([s for s in similarities if s > 0.85]),
            "recommendations": recommendations
        }
        
        return analysis
    
    def find_memo_clusters(self, memos_sample=50):
        """
        å‘ç°å¤‡å¿˜å½•çš„èšç±»å…³ç³»
        
        Args:
            memos_sample: è¦åˆ†æçš„å¤‡å¿˜å½•æ ·æœ¬æ•°é‡
        """
        # è·å–ä¸€äº›å¤‡å¿˜å½•æ ·æœ¬
        all_memos = self.get_all_memos()
        
        if len(all_memos) > memos_sample:
            # å–æœ€æ–°çš„Næ¡å¤‡å¿˜å½•ä½œä¸ºæ ·æœ¬
            sample_memos = all_memos[:memos_sample]
        else:
            sample_memos = all_memos
        
        print(f"ğŸ” åˆ†æ {len(sample_memos)} æ¡å¤‡å¿˜å½•çš„èšç±»å…³ç³»...")
        
        clusters = {}
        
        for i, memo in enumerate(sample_memos):
            memo_slug = memo.get("slug")
            print(f"åˆ†æå¤‡å¿˜å½• {i+1}/{len(sample_memos)}: {memo_slug}")
            
            # è·å–æ¨è
            analysis = self.analyze_memo_relationships(memo_slug)
            
            if analysis:
                # æ‰¾å‡ºé«˜ç›¸ä¼¼åº¦çš„å¤‡å¿˜å½•
                high_sim_memos = []
                for rec in analysis["recommendations"]:
                    if float(rec["similarity"]) > 0.85:  # é«˜ç›¸ä¼¼åº¦é˜ˆå€¼
                        high_sim_memos.append({
                            "slug": rec["memo"]["slug"],
                            "similarity": rec["similarity"],
                            "tags": rec["memo"].get("tags", [])
                        })
                
                if high_sim_memos:
                    clusters[memo_slug] = {
                        "memo_info": {
                            "slug": memo_slug,
                            "content_preview": memo.get("content", "")[:100],
                            "tags": memo.get("tags", []),
                            "created_at": memo.get("created_at")
                        },
                        "related_memos": high_sim_memos,
                        "analysis": analysis
                    }
            
            # é¿å…è¯·æ±‚è¿‡å¿«
            if i < len(sample_memos) - 1:
                time.sleep(1)
        
        print(f"âœ… å‘ç° {len(clusters)} ä¸ªå¤‡å¿˜å½•èšç±»")
        return clusters
    
    def export_relationship_network(self, clusters, filename="flomo_network.json"):
        """å¯¼å‡ºå¤‡å¿˜å½•å…³ç³»ç½‘ç»œ"""
        network_data = {
            "nodes": [],
            "edges": [],
            "metadata": {
                "export_time": datetime.now().isoformat(),
                "total_clusters": len(clusters),
                "total_nodes": 0,
                "total_edges": 0
            }
        }
        
        # æ”¶é›†æ‰€æœ‰èŠ‚ç‚¹
        all_slugs = set()
        
        for main_slug, cluster_data in clusters.items():
            all_slugs.add(main_slug)
            for related in cluster_data["related_memos"]:
                all_slugs.add(related["slug"])
        
        # åˆ›å»ºèŠ‚ç‚¹
        for slug in all_slugs:
            node = {"id": slug, "type": "memo"}
            
            # å¦‚æœæ˜¯ä¸»èŠ‚ç‚¹ï¼Œæ·»åŠ æ›´å¤šä¿¡æ¯
            if slug in clusters:
                cluster_data = clusters[slug]
                node.update({
                    "content_preview": cluster_data["memo_info"]["content_preview"],
                    "tags": cluster_data["memo_info"]["tags"],
                    "created_at": cluster_data["memo_info"]["created_at"],
                    "is_main_node": True
                })
            else:
                node["is_main_node"] = False
            
            network_data["nodes"].append(node)
        
        # åˆ›å»ºè¾¹
        for main_slug, cluster_data in clusters.items():
            for related in cluster_data["related_memos"]:
                edge = {
                    "source": main_slug,
                    "target": related["slug"],
                    "similarity": float(related["similarity"]),
                    "type": "similarity"
                }
                network_data["edges"].append(edge)
        
        # æ›´æ–°å…ƒæ•°æ®
        network_data["metadata"]["total_nodes"] = len(network_data["nodes"])
        network_data["metadata"]["total_edges"] = len(network_data["edges"])
        
        # å¯¼å‡ºåˆ°æ–‡ä»¶
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(network_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… å…³ç³»ç½‘ç»œå·²å¯¼å‡ºåˆ° {filename}")
        return network_data

def main():
    # é…ç½®token
    TOKEN = "Bearer 6782846|pguJkOHgJ21KYW4oHfrEF0syJvHRygKI5a53Mitf"
    
    api = FlomoCompleteAPI(TOKEN)
    
    print("ğŸš€ Flomo å®Œæ•´ API å®¢æˆ·ç«¯æ¼”ç¤º")
    print("="*60)
    
    # 1. æµ‹è¯•è·å–å•ä¸ªå¤‡å¿˜å½•çš„æ¨è
    print("\n1ï¸âƒ£ æµ‹è¯•å¤‡å¿˜å½•æ¨èåŠŸèƒ½")
    test_slug = "MTUyNzU0MDU3"  # ä½¿ç”¨ä½ æä¾›çš„ç¤ºä¾‹slug
    analysis = api.analyze_memo_relationships(test_slug)
    
    if analysis:
        print(f"ğŸ“Š æ¨èåˆ†æç»“æœ:")
        print(f"   æ€»æ¨èæ•°: {analysis['total_recommendations']}")
        print(f"   ç›¸ä¼¼åº¦èŒƒå›´: {analysis['similarity_stats']['min']:.3f} - {analysis['similarity_stats']['max']:.3f}")
        print(f"   å¹³å‡ç›¸ä¼¼åº¦: {analysis['similarity_stats']['avg']:.3f}")
        print(f"   é«˜ç›¸ä¼¼åº¦(>0.85): {analysis['high_similarity_count']} æ¡")
        print(f"   çƒ­é—¨æ ‡ç­¾: {list(analysis['top_tags'].keys())[:5]}")
    
    # 2. å‘ç°å¤‡å¿˜å½•èšç±»
    print(f"\n2ï¸âƒ£ å¤‡å¿˜å½•èšç±»åˆ†æ")
    clusters = api.find_memo_clusters(memos_sample=10)  # å°æ ·æœ¬æµ‹è¯•
    
    if clusters:
        print(f"ğŸ“ˆ èšç±»åˆ†æç»“æœ:")
        for slug, cluster in list(clusters.items())[:3]:  # æ˜¾ç¤ºå‰3ä¸ªèšç±»
            print(f"\n   èšç±»ä¸­å¿ƒ: {slug}")
            print(f"   å†…å®¹é¢„è§ˆ: {cluster['memo_info']['content_preview']}...")
            print(f"   ç›¸å…³å¤‡å¿˜å½•: {len(cluster['related_memos'])} æ¡")
            print(f"   æ ‡ç­¾: {cluster['memo_info']['tags']}")
    
    # 3. å¯¼å‡ºå…³ç³»ç½‘ç»œ
    print(f"\n3ï¸âƒ£ å¯¼å‡ºå…³ç³»ç½‘ç»œ")
    if clusters:
        network = api.export_relationship_network(clusters)
        print(f"ğŸŒ ç½‘ç»œç»Ÿè®¡:")
        print(f"   èŠ‚ç‚¹æ•°: {network['metadata']['total_nodes']}")
        print(f"   è¾¹æ•°: {network['metadata']['total_edges']}")
    
    print(f"\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")

if __name__ == "__main__":
    main()